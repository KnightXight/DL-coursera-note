{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep L-layer Neural Network\n",
    "\n",
    "Logistic regression often has only one layer, which is output logistic layer. However, deep neural network has more than one layer.\n",
    "\n",
    "### Notations\n",
    "\n",
    "$$ n^{[l]}: \\text{number of units in layer } l $$\n",
    "$$ a^{[l]}: \\text{activation of layer } l $$\n",
    "$$ a^{[0]} = x: \\text{input} $$\n",
    "$$ a^{[1]} = g^{[1]}(z^{[1]}) $$\n",
    "$$ a^{[L]} = \\hat{y}: \\text{output} $$\n",
    "$$ W^{[l]}: \\text{weight matrix of layer } l $$\n",
    "$$ b^{[l]}: \\text{bias vector of layer } l $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in a Deep Network\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\tag{1}$$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) \\tag{2}$$\n",
    "$$ \\hat{Y} = A^{[L]} = g^{[L]}(Z^{[L]}) \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting matrix dimisions right**\n",
    "\n",
    "for a NN has the following dimensions:\\\n",
    "L=5\n",
    "\n",
    "$n^{[l]}$ | number of units in layer l\n",
    "-|-\n",
    "$n^{[0]}$ | $n_x$ = 2\n",
    "$n^{[1]}$ | 3\n",
    "$n^{[2]}$ | 5\n",
    "$n^{[3]}$ | 4\n",
    "$n^{[4]}$ | 2\n",
    "$n^{[5]}$ | 1\n",
    "\n",
    "$ Z^{[1]}=W^{[1]}X+b^{[1]}$\n",
    "\n",
    "$Z^{[1]}$ | $W^{[1]}$ | $X$ | $b^{[1]}$\n",
    "-|-|-|-\n",
    "$(3,1)$ | $(3,2)$ | $(2,1)$ | $(3,1)$\n",
    "\n",
    "**The most important thing is to get the dimensions of the matrices right.**\n",
    "Parameters | Dimensions\n",
    "-|-\n",
    "$Dim(X)$, $Dim(A^{[0]})$ | $(n_x,m)$\n",
    "$Dim(W^{[l]})$ | $(n^{[l]},n^{[l-1]})$\n",
    "$Dim(b^{[l]})$ | $(n^{[l]},1)$\n",
    "$Dim(dW^{[l]})$ | $(n^{[l]},n^{[l-1]})$\n",
    "$Dim(db^{[l]})$ | $(n^{[l]},1)$\n",
    "$Dim(Z^{[l]})$ | $(n^{[l]},m)$\n",
    "$Dim(A^{[l]})$ | $(n^{[l]},m)$\n",
    "$Dim(dZ^{[l]})$ | $(n^{[l]},m)$\n",
    "$Dim(dA^{[l]})$ | $(n^{[l]},m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep?\n",
    "\n",
    "Image Recognition\n",
    "- input layer (layer 0): pixels\n",
    "- hidden layer 1: edges\n",
    "- hidden layer 2: shapes\n",
    "- hidden layer 3: objects\n",
    "- hidden layer 4: faces\n",
    "- output layer: person\n",
    "\n",
    "Speech Recognition\n",
    "- input layer (layer 0): sound waves\n",
    "- hidden layer 1: low level sounds\n",
    "- hidden layer 2: phonemes\n",
    "- hidden layer 3: syllables\n",
    "- hidden layer 4: words\n",
    "- output layer: sentence\n",
    "\n",
    "Circuit Theory \n",
    "> The complexity is O(log(n)), while if one hidden layer then exponential growth\n",
    "- input layer (layer 0): transistors\n",
    "- hidden layer 1: logic gates\n",
    "- hidden layer 2: adders\n",
    "- hidden layer 3: arithmetic logic units\n",
    "- hidden layer 4: microprocessors\n",
    "- output layer: computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "+ LAYER L:\n",
    "- $n^{[l]}$ = number of units in layer l\n",
    "- $W^{[l]}$ = weight matrix of shape $(n^{[l]}, n^{[l-1]})$\n",
    "- $b^{[l]}$ = bias vector of shape $(n^{[l]}, 1)$\n",
    "\n",
    "+ FORWARD PROPAGATION:\n",
    "- $a^{[l-1]}$ = input vector of shape $(n^{[l-1]}, 1)$\n",
    "- $a^{[l]}$ = output vector of shape $(n^{[l]}, 1)$\n",
    "- $z^{[l]}$ = linear transformation of $a^{[l-1]}$ by $W^{[l]}$ and $b^{[l]}$ of shape $(n^{[l]}, 1)$\n",
    "- $g^{[l]}$ = activation function of layer l\n",
    "- cache = tuple of $(z^{[l]}, W^{[l]}, b^{[l]}, a^{[l-1]})$\n",
    "\n",
    "+ BACKWARD PROPAGATION:\n",
    "- $da^{[l]}$ = input derivative of $a^{[l]}$ with respect to the cost function\n",
    "- $da^{[l-1]}$ = output derivative of $a^{[l-1]}$ with respect to the cost function\n",
    "- $dW^{[l]}$ = output derivative of $W^{[l]}$ with respect to the cost function\n",
    "- $db^{[l]}$ = output derivative of $b^{[l]}$ with respect to the cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation\n",
    "\n",
    "Input: $A^{[l-1]}$\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "Input: $A^{[l]}$\n",
    "$$ dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]})$$\n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]}A^{[l-1]T}$$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l]T}dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Hyperparameters\n",
    "\n",
    "Parameters: Variables that determine the network's predictions. They are learned during the training process.\n",
    "\n",
    "Hyperparameters: Variables that determine the network's structure. They are set before the training process.\n",
    "\n",
    "**Applied deep learning is a very empirical process.**\n",
    "\n",
    "Idea --> Code --> Experiment --> Idea --> ...\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
