{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview \n",
    "for input $x$, output $\\hat{y}$, and weights $w$:\n",
    "\n",
    "in each neuron $j$ we compute the following:\n",
    "\n",
    "$$X=A^{[0]}$$\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "$$A^{[L]} = \\hat{y}$$\n",
    "\n",
    "for back propagation we compute the following:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]})$$\n",
    "$$dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]T}$$\n",
    "$$db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$$\n",
    "$$dA^{[l-1]} = W^{[l]T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neural Network Representation \n",
    "\n",
    "<a name='1-1'></a>\n",
    "### Example with one hidden layer:\n",
    "\n",
    "> I really don't like to put it here, as I want to depict a general form.\n",
    "\n",
    "input layer = layer zero\n",
    "hidden layer [k] = layer k+1\n",
    "output layer = layer $\\sum{k}$\n",
    "\n",
    "### Compute Details\n",
    "\n",
    "for each node:\n",
    "\n",
    "$$z^{[l]}_{i} = W^{[l]}_{i}a^{[l-1]}_{i} + b^{[l]}_{i}$$\n",
    "$$a^{[l]}_{i} = g^{[l]}(z^{[l]}_{i})$$\n",
    "\n",
    "### Vectorized Form\n",
    "\n",
    "X = [x1, x2, ..., xn] # X is a (nx,m) matrix, xn is Column Vector.\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "### Explanation for Vectorized Implementation\n",
    "\n",
    "for each z calculation:\n",
    "\n",
    "$$z^{(1)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "stacking the training examples horizontally:\n",
    "\n",
    "$$Z^{(1)} = W^{(1)}X + b^{(1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "While $Z=WX+b$\n",
    "\n",
    "Activation | g(Z) | g'(Z)\n",
    "-|-|-\n",
    "Linear: | $Z$ | $1$\n",
    "Sigmoid: | $\\frac {1}{1+e^{-Z}}$ | $g(Z) * (1 - g(Z))$\n",
    "tanh: | $\\frac {e^Z - e^{-Z}}{e^Z + e^{-Z}}$ | $1 - g(Z)^2$\n",
    "ReLU: | $max(0, Z)$ | if $Z > 0$: $1$, else: $0$\n",
    "Leaky ReLU: | $max(0.01Z, Z)$ | if $Z > 0$: $1$, else: $0.01$\n",
    "softmax: | $e^Z / \\sum e^Z$ | $g(Z) * (1 - g(Z))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Forward propagation:\n",
    "$$ Z^{[l]} = W^{[l]} X + b^{[l]} \\tag{1}$$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) \\tag{2}$$\n",
    "$$ L(\\hat{y}^{(i)}, y^{(i)}) = - y^{(i)} \\log(\\hat{y}^{(i)}) - (1-y^{(i)}) \\log(1-\\hat{y}^{(i)}) \\tag{3}$$\n",
    "\n",
    "### Cost function: \n",
    "$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}, b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}) \\tag{4}$$\n",
    "\n",
    "### Backward propagation:\n",
    "$$ dZ^{[L]} = A^{[L]} - Y \\tag{1}$$\n",
    "$$ dW^{[L]} = \\frac{1}{m} dZ^{[L]} A^{[L-1]T} \\tag{2}$$\n",
    "$$ db^{[L]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[L](i)} \\tag{3}$$\n",
    "$$ dZ^{[l]} = W^{[l+1]T} dZ^{[l+1]} * g^{[l]'}(Z^{[l]}) \\tag{4}$$ \n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]T} \\tag{5}$$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)} \\tag{6}$$\n",
    "\n",
    "### Update parameters: \n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}\\tag{1}$$\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial J}{\\partial b^{[l]}}\\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initializing to zeros:\n",
    "```python\n",
    "w = np.zeros((dim, 1))\n",
    "b = 0\n",
    "w=[[0.]\n",
    " [0.]]\n",
    "```\n",
    "W is always the same, so the back propagation has the same result. We have **Symmetric Neurons**.\n",
    "\n",
    "Accordingly, we should apply random initialization.\\\n",
    "Initializing randomly\n",
    "```python\n",
    "w = np.random.randn((dim, nodes)) * 0.01\n",
    "b = np.zeros((nodes,1))\n",
    "# w=[[0.01788628]\n",
    "#    [0.0043651 ]]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
