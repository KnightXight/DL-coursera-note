{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Basics\n",
    "\n",
    "> Week2\n",
    "\n",
    "- [Logistic Regression as a Neural Network](#logistic-regression-as-a-neural-network)\n",
    "  - [Binary Classification](#binary-classification)\n",
    "  - [Logistic Regression](#logistic-regression)\n",
    "  - [Logistic Regression Cost Function](#logistic-regression-cost-function)\n",
    "  - [Gradient Descent](#gradient-descent)\n",
    "  - [Computation Graphs](#computation-graphs)\n",
    "  - [Logistic Regression Gradient Descent](#logistic-regression-gradient-descent)\n",
    "  - [Gradient Descent on m Examples](#gradient-descent-on-m-examples)\n",
    "- [Python and Vectorization](#python-and-vectorization)\n",
    "  - [Vectorization](#vectorization)\n",
    "  - [Examples](#examples)\n",
    "  - [Vectorizing Logistic Regression](#vectorizing-logistic-regression)\n",
    "  - [Vectorizing Gradient](#vectorizing-gradient)\n",
    "  - [Broadcasting in Python](#broadcasting-in-python)\n",
    "  - [Note on Vectors](#note-on-vectors)\n",
    "  - [Logistic Regression Cost Function](#logistic-regression-cost-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "Notation\n",
    "E.g. Cat vs. Non-Cat\\\n",
    "for 1(cat) vs 0(non-cat)\\\n",
    "image shape: (num_px, num_px, color) = (height, width, color)\\\n",
    "* num_px = 64, 64x64 image\n",
    "* color = 3, RGB\n",
    "\n",
    "x=[x1,x2,...,xn] is a vector of features\\\n",
    "y is a label\n",
    "* in this case nx = 64x64x3 = 12288\n",
    "\n",
    "m is the number of training examples\n",
    "* m:{(x1,y1),(x2,y2),...,(xm,ym)}\n",
    "\n",
    "X is a matrix of features\n",
    "- X=[x1,x2,...,xm], X.shape = (nx,m)\n",
    "\n",
    "Y is a vector of labels\n",
    "- Y=[y1,y2,...,ym], Y.shape = (1,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "> Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "Given x, y_hat=P(y=1|x), y_hat = sigmoid(w^T@x + b), where sigmoid(z) = 1/(1+e^{-z})\\\n",
    "x is Rnx, w is Rnx, b is R, y_hat is R, y is R, P(y=1|x) is R, sigmoid(z) is R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function\n",
    "\n",
    "$\\hat{y} = g(z)$\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$z(x)=w^Tx+b$\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})$\n",
    "\n",
    "$L(\\hat{y},y) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$ (for $y\\in\\{0,1\\}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "> Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n",
    "\n",
    "repeat until convergence: {\\\n",
    "\n",
    "$w=w-Î±\\frac{\\delta J(w,b)}{\\delta w}$\\\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graphs\n",
    "> A computation graph is a way of writing a mathematical expression as a graph. It is composed of nodes and edges. An edge from node A to node B means that the value of node A is an input to node B. A node can be a variable, a constant, or an operation. A computation graph can be evaluated to compute the value of each node.\n",
    "\n",
    "E.g. $J(x,y,z) = 3(a+bc)$ as $u=bc, v=a+u, J=3v$\n",
    "\n",
    "for a=5, b=3, c=2, u=bc=6, v=a+u=11, J=3v=33\n",
    "\n",
    "$\\frac{dJ}{dv}=3, \\frac{dv}{da}=1, \\frac{dJ}{da}=\\frac{dJ}{dv}\\frac{dv}{da}=3$\n",
    "\n",
    "$\\frac{dJ}{du}=3, \\frac{du}{db}=c, \\frac{dJ}{db}=\\frac{dJ}{du}\\frac{du}{db}=3c=6$\n",
    "\n",
    "c the same as b\n",
    "\n",
    "quote: actually it is chain rule in calculus but with a back propagation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient Descent\n",
    "\n",
    "e.g. \n",
    "\n",
    "we have $x^{(n)}, w^{(n)}, b$\n",
    "\n",
    "$z=w^{(n)}x^{(n)}+b$\n",
    "\n",
    "$\\hat{y}=\\sigma(z)$\n",
    "\n",
    "$L(\\hat{y},y)=-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$\n",
    "\n",
    "- back prop\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\hat{y}}=-\\frac{y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}$\n",
    "\n",
    "$\\frac{\\partial \\hat{y}}{\\partial z}=\\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w^{(i)}}=x^{(i)}$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial b}=1$\n",
    "\n",
    "- chain rule applied\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z}=\\hat{y}-y$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w^{(i)}}=x^{(i)}(\\hat{y}-y)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}=\\hat{y}-y$\n",
    "\n",
    "- update\n",
    "\n",
    "$w^{(i)}=w^{(i)}-\\alpha\\frac{\\partial L}{\\partial w^{(i)}}$\n",
    "\n",
    "$b=b-\\alpha\\frac{\\partial L}{\\partial b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent on m examples\n",
    "```python\n",
    "for i in range(0, m):\n",
    "    z[i] = np.dot(w.T, x[i]) + b\n",
    "    a[i] = sigmoid(z[i])\n",
    "    J+= -y[i] * np.log(a[i]) - (1 - y[i]) * np.log(1 - a[i])\n",
    "    dz[i] = a[i] - y[i]\n",
    "    dw += x[i] * dz[i]\n",
    "    db += dz[i]\n",
    "J /= m\n",
    "dw /= m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "- Vectorization is the art of getting rid of explicit for-loops in code.\n",
    "- Vectorization is important in deep learning because it provides computational efficiency as applying parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-vectorized version:573.4927654266357ms\n",
      "vectorized version:1.3382434844970703ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "w=np.random.rand(1000000)\n",
    "x=np.random.rand(1000000)\n",
    "\n",
    "#non-vectorized implementation:\n",
    "tic=time.time()\n",
    "z=0\n",
    "for i in range(len(w)):\n",
    "    z+=w[i]*x[i]\n",
    "toc=time.time()\n",
    "print(\"non-vectorized version:\"+str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "#vectorized implementation:\n",
    "tic=time.time()\n",
    "z=np.dot(w,x)\n",
    "toc=time.time()\n",
    "print(\"vectorized version:\"+str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "# expected output:\n",
    "# it changes every time you run it, but the point is the gap between\n",
    "# non-vectorized version:578.0763626098633ms\n",
    "# vectorized version:1.3744831085205078ms\n",
    "# vectorized version is 420 times faster than non-vectorized version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "```python\n",
    "import numpy as np\n",
    "dw=np.zeros((n_x,1))\n",
    "u=np.exp(v)\n",
    "u=np.log(u)\n",
    "u=np.maximum(0,v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Logistic Regression\n",
    "\n",
    "```python\n",
    "# Z is a 1*m vector, where m is the number of training examples\n",
    "# w is a n*1 vector, where n is the number of features\n",
    "# X is a n*m matrix, where n is the number of features\n",
    "# b is a scalar\n",
    "# A is a 1*m vector, where m is the number of training examples\n",
    "Z=w.T*X+b\n",
    "A=sigmoid(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Gradient\n",
    "\n",
    "```python\n",
    "# Y is a 1*m vector, A is a 1*m vector, dZ is a 1*m vector\n",
    "dZ=A-Y\n",
    "db=1/m*sum(dZ)\n",
    "dw=1/m*X*dZ.T\n",
    "w=w-alpha*dw\n",
    "b=b-alpha*db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting in Python\n",
    "\n",
    "> Broadcasting is a very useful feature in Python that allows us to perform operations on two arrays/ tensors, even if these arrays/ tensors do not have the same shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "[15 18 21 24]\n",
      "(4,)\n",
      "[[ 6.66666667 11.11111111 14.28571429 16.66666667]\n",
      " [33.33333333 33.33333333 33.33333333 33.33333333]\n",
      " [60.         55.55555556 52.38095238 50.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([[1,2,3,4],\n",
    "            [5,6,7,8],\n",
    "            [9,10,11,12]])\n",
    "print(A)\n",
    "cal = A.sum(axis=0) # sum of each column\n",
    "print(cal)\n",
    "print(cal.shape)\n",
    "percentage = 100*A/cal.reshape(1,4) # reshape(1,4) is to make cal become a row vector\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where A is a (3,4) matrix, cal (1,4)\\\n",
    "percentage is (3,4) divided by (1,4)\n",
    "\n",
    "**General**\n",
    "- (m,n) + (1,n) = (m,n) + (m,n)\n",
    "- (m,n) + (m,1) = (m,n) + (m,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Vectors\n",
    "use randn(5,1) instead of (5)\\\n",
    "the former one is a rank 2 column vector\\\n",
    "randn(1,5) is row vector\\\n",
    "assert(a.shape == (5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.05704483  1.52958024 -0.766576   -0.61909726  0.76353203]\n",
      "(5,)\n",
      "[-1.05704483  1.52958024 -0.766576   -0.61909726  0.76353203]\n",
      "5.010860818185966\n",
      "[[ 0.50686633]\n",
      " [ 1.48000822]\n",
      " [-1.19011419]\n",
      " [-1.09778839]\n",
      " [-0.66861646]]\n",
      "(5, 1)\n",
      "[[ 0.50686633  1.48000822 -1.19011419 -1.09778839 -0.66861646]]\n",
      "[[ 0.25691348  0.75016634 -0.60322881 -0.55643197 -0.33889917]\n",
      " [ 0.75016634  2.19042433 -1.76137878 -1.62473583 -0.98955786]\n",
      " [-0.60322881 -1.76137878  1.41637178  1.30649353  0.79572994]\n",
      " [-0.55643197 -1.62473583  1.30649353  1.20513934  0.73399939]\n",
      " [-0.33889917 -0.98955786  0.79572994  0.73399939  0.44704797]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.random.randn(5)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.T)\n",
    "print(np.dot(a,a.T))\n",
    "a=np.random.randn(5,1)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.T)\n",
    "print(np.dot(a,a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function\n",
    "\n",
    "$z(x)=wx+b$\n",
    "\n",
    "$/sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$\\hat{y}=\\sigma(z)$\n",
    "\n",
    "we interpret $\\hat{y}$ as the probability that $y=1$ given $x$.\n",
    "\n",
    "$P(y=1|x)=\\hat{y}$\n",
    "\n",
    "$P(y=0|x)=1-\\hat{y}$\n",
    "\n",
    "$P(y|x)=\\hat{y}^{y}(1-\\hat{y})^{(1-y)}$\n",
    "\n",
    "$L(y)=logP(y|x)=ylog\\hat{y}+(1-y)log(1-\\hat{y})$\n",
    "\n",
    "$P(\"labels in traning set\")=log\\prod_{i=1}^{m}P(y^{(i)}|x^{(i)})$\n",
    "\n",
    "Due to Maximum liklihood estimation, we want to maximize the above equation.\n",
    "\n",
    "$J(w,b)=-\\frac{1}{m}log\\prod_{i=1}^{m}P(y^{(i)}|x^{(i)})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
