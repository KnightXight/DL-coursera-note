{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "### Notation\n",
    "E.g. Cat vs. Non-Cat\\\n",
    "for 1(cat) vs 0(non-cat)\\\n",
    "image shape: (num_px, num_px, color) = (height, width, color)\\\n",
    "* num_px = 64, 64x64 image\n",
    "* color = 3, RGB\n",
    "\n",
    "x=[x1,x2,...,xn] is a vector of features\\\n",
    "y is a label\n",
    "* in this case nx = 64x64x3 = 12288\n",
    "\n",
    "m is the number of training examples\n",
    "* m:{(x1,y1),(x2,y2),...,(xm,ym)}\n",
    "\n",
    "X is a matrix of features\n",
    "- X=[x1,x2,...,xm], X.shape = (nx,m)\n",
    "\n",
    "Y is a vector of labels\n",
    "- Y=[y1,y2,...,ym], Y.shape = (1,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "> Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "Given x, y_hat=P(y=1|x), y_hat = sigmoid(w^T@x + b), where sigmoid(z) = 1/(1+e^{-z})\\\n",
    "x is Rnx, w is Rnx, b is R, y_hat is R, y is R, P(y=1|x) is R, sigmoid(z) is R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function\n",
    "- $\\hat{y} = g(z)$\n",
    "- $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "- $z(x)=w^Tx+b$\n",
    "- $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})$\n",
    "- $L(\\hat{y},y) = -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$ (for $y\\in\\{0,1\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "> Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n",
    "\n",
    "repeat until convergence: {\\\n",
    "$w=w-Î±\\frac{\\delta J(w,b)}{\\delta w}$\\\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graphs\n",
    "> A computation graph is a way of writing a mathematical expression as a graph. It is composed of nodes and edges. An edge from node A to node B means that the value of node A is an input to node B. A node can be a variable, a constant, or an operation. A computation graph can be evaluated to compute the value of each node.\n",
    "\n",
    "E.g. $J(x,y,z) = 3(a+bc)$ as $u=bc, v=a+u, J=3v$\\\n",
    "for a=5, b=3, c=2, u=bc=6, v=a+u=11, J=3v=33\\\n",
    "$\\frac{dJ}{dv}=3, \\frac{dv}{da}=1, \\frac{dJ}{da}=\\frac{dJ}{dv}\\frac{dv}{da}=3$\\\n",
    "$\\frac{dJ}{du}=3, \\frac{du}{db}=c, \\frac{dJ}{db}=\\frac{dJ}{du}\\frac{du}{db}=3c=6$\\\n",
    "c the same as b\n",
    "\\\n",
    "quote: actually it is chain rule in calculus but with a back propagation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient Descent\n",
    "- e.g. we have $x^{(n)}, w^{(n)}, b$\n",
    "- $z=w^{(n)}x^{(n)}+b$\n",
    "- $\\hat{y}=\\sigma(z)$\n",
    "- $L(\\hat{y},y)=-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$\n",
    "* back prop\n",
    "- $\\frac{\\partial L}{\\partial \\hat{y}}=-\\frac{y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}$\n",
    "- $\\frac{\\partial \\hat{y}}{\\partial z}=\\hat{y}(1-\\hat{y})$\n",
    "- $\\frac{\\partial z}{\\partial w^{(i)}}=x^{(i)}$\n",
    "- $\\frac{\\partial z}{\\partial b}=1$\n",
    "* chain rule applied\n",
    "- $\\frac{\\partial L}{\\partial z}=\\hat{y}-y$\n",
    "- $\\frac{\\partial L}{\\partial w^{(i)}}=x^{(i)}(\\hat{y}-y)$\n",
    "- $\\frac{\\partial L}{\\partial b}=\\hat{y}-y$\n",
    "* update\n",
    "- $w^{(i)}=w^{(i)}-\\alpha\\frac{\\partial L}{\\partial w^{(i)}}$\n",
    "- $b=b-\\alpha\\frac{\\partial L}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```to be continued```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
