{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Practical Aspects of Deep Learning\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up your Machine Learning Application\n","\n","### Train / Dev / Test sets\n","\n","Iterative process:\\\n","layers\\\n","hidden units\\\n","learning rate\\\n","activation functions\\\n","train/dev/test sets\n","\n","Data:\\\n","train set: 70%\\\n","Hold-out cross validation/development dev set: 30%\\\n","test set: 30%\n","\n","For big data (1M+):\\\n","train set: 98%\\\n","dev/test set: 1%\n","\n","### Bias / Variance\n","\n","High bias (underfitting):\\\n","train set error: 15%\\\n","dev set error: 14%\\\n","high error on both sets\n","\n","High variance (overfitting):\\\n","train set error: 1%\\\n","dev set error: 11%\\\n","low error on train set, high error on dev set\n","\n","High bias and high variance:\\\n","train set error: 15%\\\n","dev set error: 30%\\\n","high error on both sets\n","\n","Low bias and low variance:\\\n","train set error: 0.5%\\\n","dev set error: 1%\\\n","low error on both sets\n","\n","### Basic Recipe for Machine Learning\n","\n","High bias (underfitting):\n","- bigger network\n","- train longer\n","- NN architecture search\n","- Adam\n","- RMSprop\n","\n","High variance (overfitting):\n","- more data\n","- regularization\n","- NN architecture search\n","- Adam\n","- RMSprop\n","- dropout\n","- data augmentation\n","- early stopping\n","- batch normalization\n","- L2 regularization\n"]},{"cell_type":"markdown","metadata":{},"source":["## Regularizing your Neural Network\n","\n","### Regularization\n","\n","+ Regularization\n","$$ min_{W, b} $$\n","$$J(w,b)=\\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} ||W^{[l]}||_{F}^{2}$$\n","$$\\text{Frobenius norm: }||W^{[l]}||_{F}^{2} = \\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}} (W_{ij}^{[l]})^{2} $$\n","$$ dW^{[l]} = \\text{from backprop} + \\frac{\\lambda}{2m}W^{[l]} $$\n","\n","### Why Regularization Helps\n","\n","+ Intuition\n","  + If $\\lambda$ is too large, the network will be more linear\n","  + If $\\lambda$ is good, the network will be less prone to overfitting\n","  + $\\lambda$ is a hyperparameter that you can tune using a dev set\n","\n","\n","### Dropout Regularization\n","\n","+ Dropout\n","  + At each iteration, shut down each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$\n","  + At test time, don't do anything (don't apply dropout)\n","+ Implementation details\n","  + At test time, multiply each dropout layer by keep_prob to keep the same expected value for the activations\n","  + Ex. $a^{[3]} = np.random.rand(a^{[3]}.shape[0], a^{[3]}.shape[1]) < keep_prob$\n","  + $a^{[3]} = np.multiply(a^{[3]}, d^{[3]})$\n","  + $a^{[3]} /= keep_prob$\n","\n","### Understanding Dropout\n","\n","+ Intuition\n","  + Dropout randomly shuts down some neurons on each iteration\n","  + $d^{[3]}$ is a matrix of the same shape as $a^{[3]}$\n","  + Each entry of $d^{[3]}$ is 0 or 1 with equal probability\n","  + Ex. $keep\\_prob = 0.8 \\rightarrow$ 80% chance that a neuron will be kept\n","  + $\\rightarrow$ At test time, multiply by 0.8 to keep the same expected value for the activations\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
