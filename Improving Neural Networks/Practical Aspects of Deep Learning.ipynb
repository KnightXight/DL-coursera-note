{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Practical Aspects of Deep Learning\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up your Machine Learning Application\n","\n","### Train / Dev / Test sets\n","\n","Iterative process:\\\n","layers\\\n","hidden units\\\n","learning rate\\\n","activation functions\\\n","train/dev/test sets\n","\n","Data:\\\n","train set: 70%\\\n","Hold-out cross validation/development dev set: 30%\\\n","test set: 30%\n","\n","For big data (1M+):\\\n","train set: 98%\\\n","dev/test set: 1%\n","\n","### Bias / Variance\n","\n","High bias (underfitting):\\\n","train set error: 15%\\\n","dev set error: 14%\\\n","high error on both sets\n","\n","High variance (overfitting):\\\n","train set error: 1%\\\n","dev set error: 11%\\\n","low error on train set, high error on dev set\n","\n","High bias and high variance:\\\n","train set error: 15%\\\n","dev set error: 30%\\\n","high error on both sets\n","\n","Low bias and low variance:\\\n","train set error: 0.5%\\\n","dev set error: 1%\\\n","low error on both sets\n","\n","### Basic Recipe for Machine Learning\n","\n","High bias (underfitting):\n","- bigger network\n","- train longer\n","- NN architecture search\n","- Adam\n","- RMSprop\n","\n","High variance (overfitting):\n","- more data\n","- regularization\n","- NN architecture search\n","- Adam\n","- RMSprop\n","- dropout\n","- data augmentation\n","- early stopping\n","- batch normalization\n","- L2 regularization\n"]},{"cell_type":"markdown","metadata":{},"source":["## Regularizing your Neural Network\n","\n","### Regularization\n","\n","+ Regularization\n","$$ min_{W, b} $$\n","$$J(w,b)=\\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} ||W^{[l]}||_{F}^{2}$$\n","$$\\text{Frobenius norm: }||W^{[l]}||_{F}^{2} = \\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}} (W_{ij}^{[l]})^{2} $$\n","$$ dW^{[l]} = \\text{from backprop} + \\frac{\\lambda}{2m}W^{[l]} $$\n","\n","### Why Regularization Helps\n","\n","+ Intuition\n","  + If $\\lambda$ is too large, the network will be more linear\n","  + If $\\lambda$ is good, the network will be less prone to overfitting\n","  + $\\lambda$ is a hyperparameter that you can tune using a dev set\n","\n","\n","### Dropout Regularization\n","\n","+ Dropout\n","  + At each iteration, shut down each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$\n","  + At test time, don't do anything (don't apply dropout)\n","+ Implementation details\n","  + At test time, multiply each dropout layer by keep_prob to keep the same expected value for the activations\n","  + Ex. $a^{[3]} = np.random.rand(a^{[3]}.shape[0], a^{[3]}.shape[1]) < keep_prob$\n","  + $a^{[3]} = np.multiply(a^{[3]}, d^{[3]})$\n","  + $a^{[3]} /= keep_prob$\n","\n","### Understanding Dropout\n","\n","+ Intuition\n","  + Dropout randomly shuts down some neurons on each iteration\n","  + $d^{[3]}$ is a matrix of the same shape as $a^{[3]}$\n","  + Each entry of $d^{[3]}$ is 0 or 1 with equal probability\n","  + Ex. $keep\\_prob = 0.8 \\rightarrow$ 80% chance that a neuron will be kept\n","  + $\\rightarrow$ At test time, multiply by 0.8 to keep the same expected value for the activations\n"]},{"cell_type":"markdown","metadata":{},"source":["## Other Regularization Methods\n","\n","### Data Augmentation\n","- Mirroring\n","- Random Cropping\n","- Rotation\n","- Shearing\n","- Local Warping\n","- Color Shifting\n","- ...\n","- Applying above methods to training set, but not to dev/test sets\n","- The augmented data must still be reasonable\n","- The augmented data must be label preserving\n","- The augmented data must be cheap to generate\n","- The augmented data must not hurt performance of the model\n","- The augmented data must be generated by a computer\n","- The augmented data must not be too easy to classify\n","- The augmented data must not be too difficult to classify\n","- ...\n","- State-of-the-art object detection algorithms use data augmentation\n","- State-of-the-art speech recognition algorithms use data augmentation\n","- State-of-the-art image recognition algorithms use data augmentation\n","\n","### Early Stopping  \n","- Stop training when the error on the dev set starts to increase (after some epoch)\n","- It is a regularization technique\n","\n","### Orthogonalization\n","Optimizing cost: Tring Everything to decrease J(w,b)\n","Not overfit: Regularization"]},{"cell_type":"markdown","metadata":{},"source":["## Setting Up your Optimization Methods\n","\n","### Normalizing Inputs\n","\n","- Subtract mean:\n","$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}$$\n","$$x := x - \\mu$$\n","- Normalize variance:\n","$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)^2}$$\n","$$x := \\frac{x}{\\sigma^2}$$\n","> Remember to use the same $\\mu$ and $\\sigma^2$ for training and testing sets.\n","> The normalized inputs should have zero mean and equal variance. The distribution should be similar to a Gaussian distribution. So that the cost function will be more symmetric and easier to optimize.\n","\n","### Vanishing / Exploding Gradients\n","\n","For a large NN with L layers with equal w, b=0:\n","\n","$$\\hat{y} = w^{L}a^{[0]}$$\n","\n","If $w>1$, then $\\hat{y}$ will explode. If $w<1$, then $\\hat{y}$ will vanish.\n","\n","### Weight Initialization for Deep Networks\n","\n","For a single neuron:\n","\n","$$z^{[l]} = W^{[l]}A^{[l-1]}$$\n","\n","If $W^{[l]}$ and $A^{[l-1]}$ are large, then $z^{[l]}$ will be large, and the activation function will be in the flat region, which will slow down the learning.\n","\n","So we want wi to be small.\n","\n","n=$n^{[l-1]}$\n","Var(W) = 1/n\n","$W^{[l]}$ = np.random.randn(shape) * np.sqrt(1/n)\n","\n","If ReLU is used, then $W^{[l]}$ = np.random.randn(shape) * np.sqrt(2/n)\n","\n","If tanh is used, then $W^{[l]}$ = np.random.randn(shape) * np.sqrt(1/n)\n","\n","Xavier initialization: np.random.randn(shape) * np.sqrt(1/(n[l-1]+n[l]))\n","\n","### Numerical Approximation of Gradients\n","\n","$$ \\frac{f( \\theta + \\epsilon) -f( \\theta - \\epsilon)}{2 \\epsilon} = g(\\theta)$$\n","$$ f'( \\theta) \\approx g(\\theta)=\\lim_{\\epsilon \\rightarrow 0}\\frac{f( \\theta + \\epsilon) -f( \\theta - \\epsilon)}{2 \\epsilon}$$\n","\n","### Gradient Checking\n","\n","Take W[l] and b[l] and reshape them into a big vector $\\theta$.\n","\n","J($\\theta$) = J(W[1], b[1], ..., W[L], b[L])\n","\n","Take dW[l] and db[l] and reshape them into a big vector d$\\theta$.\n","\n","Is d$\\theta$ close to gradient of J($\\theta$)?\n","\n","for each i:\n","$$ d\\theta_{approx}[i] = \\frac{J(\\theta_1, \\theta_2, ..., \\theta_i + \\epsilon, ..., \\theta_n) - J(\\theta_1, \\theta_2, ..., \\theta_i - \\epsilon, ..., \\theta_n)}{2 \\epsilon}$$\n","$$ \\approx d\\theta[i] = \\frac{\\partial J}{\\partial \\theta_i}$$\n","Check ($epsilon=10^{-7}$):\n","$$ \\frac{||d\\theta_{approx} - d\\theta||_2}{||d\\theta_{approx}||_2 + ||d\\theta||_2} \\approx 10^{-7}$$\n","If the result is not close enough, then there is a bug in the backpropagation.\n","\n","### Gradient Checking Implementation Notes\n","\n","Don't use in training - only to debug.\n","\n","If algorithm fails grad check, look at components to try to identify bug.\n","\n","Remember regularization.\n","\n","Doesn't work with dropout. You can first turn off dropout, run gradient check, and then turn on dropout again.\n","\n","Run at random initialization; perhaps again after some training.\n","\n","Don't use with mini-batches. If you use mini-batches, then you have to run gradient check on each mini-batch, and then average the results."]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
