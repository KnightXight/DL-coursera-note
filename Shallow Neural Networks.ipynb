{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks\n",
    "\n",
    "- [Neural Networks Overview](#0)\n",
    "- [Neural Network Representation](#1)\n",
    "  - [Example](#1-1)\n",
    "  - [Details](#1-2)\n",
    "  - [Vectors](#1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Neural Network Overview\n",
    "\n",
    "for input $x$, output $\\hat{y}$, and weights $w$:\n",
    "\n",
    "in each neuron $j$ we compute the following:\n",
    "\n",
    "$$X=A^{[0]}$$\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "$$A^{[L]} = \\hat{y}$$\n",
    "\n",
    "for back propagation we compute the following:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]})$$\n",
    "$$dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]T}$$\n",
    "$$db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$$\n",
    "$$dA^{[l-1]} = W^{[l]T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Neural Network Representation ##\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### Example with one hidden layer:\n",
    "\n",
    "> I really don't like to put it here, as I want to depict a general form.\n",
    "\n",
    "input layer = layer zero\n",
    "hidden layer [k] = layer k+1\n",
    "output layer = layer $\\sum{k}$\n",
    "\n",
    "<a name='1-2'></a>\n",
    "### Compute Details\n",
    "\n",
    "for each node:\n",
    "\n",
    "$$z^{[l]}_{i} = W^{[l]}_{i}a^{[l-1]}_{i} + b^{[l]}_{i}$$\n",
    "$$a^{[l]}_{i} = g^{[l]}(z^{[l]}_{i})$$\n",
    "\n",
    "<a name='1-3'></a>\n",
    "### Vectorized Form\n",
    "\n",
    "X = [x1, x2, ..., xn] # X is a (nx,m) matrix, xn is Column Vector.\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "<a name ='1-4'></a>\n",
    "### Explanation for Vectorized Implementation\n",
    "\n",
    "for each z calculation:\n",
    "\n",
    "$$z^{(1)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "stacking the training examples horizontally:\n",
    "\n",
    "$$Z^{(1)} = W^{(1)}X + b^{(1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### \n",
    "g(Z) could be any activation\n",
    "\n",
    "Examples:\n",
    "While linear part $Z = W^T X + b$\n",
    "\n",
    "Activation | Function\n",
    "-|-\n",
    "Linear: | $g(Z) = Z$\n",
    "Sigmoid: | $g(Z) = 1 / (1 + e^(-Z))$\n",
    "tanh: | $g(Z) = (e^Z - e^(-Z)) / (e^Z + e^(-Z))$\n",
    "ReLU: | $g(Z) = max(0, Z)$\n",
    "Leaky ReLU: | $g(Z) = max(0.01Z, Z)$\n",
    "softmax: | $g(Z) = e^Z / \\sum e^Z$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
